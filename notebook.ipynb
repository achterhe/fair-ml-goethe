{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MOcvgp351F6"
      },
      "source": [
        "#  Investigating Fairness in Data-Driven Allocation of Public Resources\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2gKsxjA4r-b"
      },
      "source": [
        "This notebook demonstrates, through a real-world use case, how IBM's [AIF360 toolkit](https://github.com/Trusted-AI/AIF360) can be used to evaluate the fairness of an algorithm for allocating public resources. The data used was generated specifically for this workshop and therefore does not allow any conclusions to be drawn about individuals. Further assumptions that were made serve as a simplified representation of a complex problem and do not reflect the values of the presenters. \n",
        "\n",
        "First, a logistic regression is used to predict the risk of long-term unemployment of job seekers. If a predefined threshold is exceeded, i.e. if the probability of LTU is high according to the algorithm, the person will receive supportive measures from the public employment service.\n",
        "\n",
        "As a protected attribute we consider the variable gender, which is binary coded here. Therefore, in a second step, we will investigate whether the prediction for LTU differs between men and women. To do so, we evaluate the Statistical Parity Difference.\n",
        "\n",
        "Finally, we demonstrate techniques to mitigate existing biases. Here we apply reweighing, a method that assigns weights to individual instances in the training data based on frequency counts of the protected attribute and the actual outcome before classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbLTRDJkE4mO"
      },
      "source": [
        "## Install aif360"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaQBm5NqE4Df",
        "outputId": "6b38a404-7e68-4064-d464-63652e39554e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->aif360) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->aif360) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->aif360) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install aif360"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArhwAOZtAj4J"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87W-WtfAqZc"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import OrderedDict, defaultdict\n",
        "from IPython.core.display import Markdown\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "# Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fairness metrics\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "\n",
        "# Bias mitigation technique\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "# Set seed\n",
        "seed = 1\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4TBBfTXHZ3L"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, helper functions are defined that can be used on different datasets or with varying thresholds. "
      ],
      "metadata": {
        "id": "-gAfBnGlpZEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMXbMo1MhTo2"
      },
      "outputs": [],
      "source": [
        "def get_dataset_with_scores(dataset, model):\n",
        "    \"\"\"Returns a dataset with the scores predicted by the given model.\n",
        "\n",
        "    Args:\n",
        "        dataset (BinaryLabelDataset): The dataset to be predicted.\n",
        "        model(e.g. LogisticRegression): Classification model.\n",
        "        \"\"\"\n",
        "    dataset_pred = dataset.copy(deepcopy=True)\n",
        "    pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
        "    y_pred_prob = model.predict_proba(dataset.features)[:, pos_ind]\n",
        "    dataset_pred.scores = y_pred_prob.reshape(-1, 1)\n",
        "    \n",
        "    return dataset_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gZItKTIU4qO"
      },
      "outputs": [],
      "source": [
        "def get_label_at_threshold(dataset_pred, threshold):\n",
        "    \"\"\"Returns a dataset with new labels according to the specified threshold. \n",
        "    \n",
        "    The instances whose score is greater than the specified threshold are \n",
        "    assigned the favorable_label given in the BinaryLabelDataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_pred (BinaryLabelDataset): BinaryLabelDataset with predicted scores.\n",
        "        threshold (float): Threshold to assign labels from given scores.\n",
        "        \"\"\"\n",
        "    dataset_pred_thresh = dataset_pred.copy(deepcopy=True)\n",
        "    fav_inds = dataset_pred_thresh.scores > threshold\n",
        "    dataset_pred_thresh.labels[fav_inds] = dataset_pred_thresh.favorable_label\n",
        "    dataset_pred_thresh.labels[~fav_inds] = dataset_pred_thresh.unfavorable_label\n",
        "\n",
        "    return dataset_pred_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bybTI7FBlJJG"
      },
      "outputs": [],
      "source": [
        "def get_dataset_with_predictions(dataset, model, threshold):\n",
        "    \"\"\"Returns a dataset with new scores predicted by the given model and new \n",
        "    labels corresponding to the given threshold.\n",
        "\n",
        "    Args:\n",
        "        dataset (BinaryLabelDataset): The dataset to be used for prediction.\n",
        "        model (e.g. LogisticRegression): Classification model.\n",
        "        threshold (float): Threshold to assign labels from scores.\n",
        "        \"\"\"\n",
        "    dataset_pred = get_dataset_with_scores(dataset, model)\n",
        "    return get_label_at_threshold(dataset_pred, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric_arr(ax, metric_dict, metric_transf_dict=None, thresh=None, ylim=None, hline_at=None):\n",
        "    \"\"\"Plots the given metric_dict on the axis specified.\n",
        "\n",
        "    Args:\n",
        "        ax (Axes): The axis to be plottet on.\n",
        "        metric_dict (Dict): Dict with metrics as keys and list of metric value \n",
        "        at different thresholds as values.\n",
        "        metric_transf_dict (Dict, optional): Dict with metrics as keys and list of metric value mitigation at different thresholds as values.\n",
        "        thresh (float, optional): Threshold to be indicated by a dotted line in the plot.\n",
        "        ylim (tuple, optional): Set ylim for given axis.\n",
        "        hline_at (float, optional): If not None, hline will be highlighted at given value.\n",
        "        \"\"\"\n",
        "    for metric, metric_val in metric_dict.items():\n",
        "        ax.plot(thresh_arr, metric_val, label=metric)\n",
        "        if metric_transf_dict:\n",
        "            ax.plot(thresh_arr, metric_transf_dict[metric], label=f'{metric}_transf', linestyle='dashed', color=ax.get_lines()[-1].get_c())\n",
        "        ax.set_xlabel('Classification Threshold', fontsize=16, fontweight='bold')\n",
        "        ax.xaxis.set_tick_params(labelsize=14)\n",
        "        ax.yaxis.set_tick_params(labelsize=14)\n",
        "        ax.grid(True)\n",
        "\n",
        "        if hline_at is not None:\n",
        "            ax.axhline(hline_at, linewidth=2, color='k')\n",
        "\n",
        "        if thresh:\n",
        "            thresh_ind = np.where(np.round(thresh_arr, 2) == thresh)[0][0]\n",
        "            ax.axvline(np.array(thresh_arr)[thresh_ind], color='k', linestyle=':')\n",
        "\n",
        "    if ylim:\n",
        "        plt.setp([ax], ylim=ylim)\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "b2KN-DS9W1-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlKd9iAUjTmM"
      },
      "outputs": [],
      "source": [
        "def plot_metrics_over_thresholds(performance_metric_dict, fairness_metric_dict, thresh_arr, threshold):\n",
        "    \"\"\"Plots all metrics in the given metric array over all thresholds in the \n",
        "    given threshold array.\n",
        "\n",
        "    Args:\n",
        "        performance_metric_dict, fairness_metric_dict (Dict): Dict with metrics as keys and list of metric value \n",
        "        at different thresholds as values.\n",
        "        thresh_arr (Array): Array with different thresholds.\n",
        "        threshold (float, optional): Threshold to be indicated by a dotted line in the plot.\n",
        "        \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "    ax1 = plt.subplot(2, 2, 1)\n",
        "    plot_metric_arr(ax1, performance_metric_dict, thresh=threshold, ylim=(0 ,1))\n",
        "\n",
        "    min_val = min([v for val_list in fairness_metric_dict.values() for v in val_list])\n",
        "    max_val = max([v for val_list in fairness_metric_dict.values() for v in val_list])\n",
        "    custom_ylim = (round(min_val, 1)-0.1, round(max_val+0.1, 1))\n",
        "\n",
        "    ax2 = plt.subplot(2, 2, 2)\n",
        "    plot_metric_arr(ax2, fairness_metric_dict, thresh=threshold, ylim=custom_ylim, hline_at=0)\n",
        "\n",
        "    fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_metrics(dataset_true, dataset_pred, f1=True):\n",
        "    \"\"\"Returns classification metrics for two BinaryLabelDatasets. The first \n",
        "    dataset is the true one, the second includes the predicted scores.\n",
        "\n",
        "    Args:\n",
        "        dataset_true (BinaryLabelDataset): Dataset with true labels.\n",
        "        dataset_pred (BinaryLabelDataset): Dataset with predicted scores.\n",
        "        f1 (bool, optional): If True, calculate F1 score for classification.\n",
        "        \"\"\"\n",
        "    performance_metrics = OrderedDict()\n",
        "    fairness_metrics = OrderedDict()\n",
        "\n",
        "    classified_metric = ClassificationMetric(dataset_true, dataset_pred,\n",
        "                                                  unprivileged_groups=unprivileged_groups,\n",
        "                                                  privileged_groups=privileged_groups)\n",
        "    \n",
        "    if f1:\n",
        "        if classified_metric.precision() == 0.0 and classified_metric.recall() == 0.0:\n",
        "            return None\n",
        "        else:\n",
        "            f1_score = 2*((classified_metric.precision()*classified_metric.recall())/(classified_metric.precision() + classified_metric.recall()))\n",
        "            performance_metrics[\"F1 score\"] = f1_score\n",
        "    \n",
        "    accuracy = classified_metric.accuracy()           \n",
        "    spd = classified_metric.statistical_parity_difference()\n",
        "    eod = classified_metric.equal_opportunity_difference()\n",
        "\n",
        "    performance_metrics[\"Accuracy\"] = accuracy\n",
        "    fairness_metrics[\"Statistical parity difference\"] = spd\n",
        "    fairness_metrics[\"Equal opportunity difference\"] = eod\n",
        "\n",
        "    return performance_metrics, fairness_metrics"
      ],
      "metadata": {
        "id": "Rn6OIvJxfFI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q2TwdYLX9Ip"
      },
      "outputs": [],
      "source": [
        "def get_fairness_over_thresholds(dataset, model, thresh_arr, unprivileged_groups, privileged_groups, threshold=None, dataset_with_scores=None):\n",
        "    \"\"\"Plots fairness metrics over thresholds in the specified thresh_arr.\n",
        "\n",
        "    Args:\n",
        "        dataset (BinaryLabelDataset): Dataset with true labels.\n",
        "        model (LogisticRegression): Classification model.\n",
        "        thresh_arr (Array): Array with different thresholds.\n",
        "        unprivileged_groups (list(dict)): Unprivileged groups used as input for ClassificationMetric. For details, see AIF360 documentation.\n",
        "        privileged_groups (list(dict)): Privileged groups used as input for ClassificationMetric. For details, see AIF360 documentation.\n",
        "        threshold (float, optional): \n",
        "        \"\"\"\n",
        "    if not dataset_with_scores:\n",
        "        dataset_with_scores = get_dataset_with_scores(dataset, model)\n",
        "    performance_metric_dict = defaultdict(list)\n",
        "    fairness_metric_dict = defaultdict(list)\n",
        "\n",
        "    for i, thresh in enumerate(thresh_arr):\n",
        "        dataset_pred = get_label_at_threshold(dataset_with_scores, thresh)\n",
        "        performance_metrics, fairness_metrics = get_classification_metrics(dataset, dataset_pred)\n",
        "\n",
        "        for (metric, value) in performance_metrics.items():\n",
        "            performance_metric_dict[metric].append(value)\n",
        "        \n",
        "        for (metric, value) in fairness_metrics.items():\n",
        "            fairness_metric_dict[metric].append(value)\n",
        "    \n",
        "    plot_metrics_over_thresholds(performance_metric_dict, fairness_metric_dict, thresh_arr, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26msjw1MzUny"
      },
      "outputs": [],
      "source": [
        "def plot_metrics_transf_over_thresholds(perf_metric_dict, perf_metric_transf_dict, fair_metric_dict, fair_metric_transf_dict, thresh_arr, threshold):\n",
        "    \"\"\"Plots all metrics in the given metric dict and corresponding metrics for \n",
        "    mitigated dataset (metric_transf_dict) over all thresholds in the given threshold array.\n",
        "\n",
        "    Args:\n",
        "        perf_metric_dict, fair_metric_dict (Dict): Dict with metrics as keys and list of metric value at different thresholds as values.\n",
        "        perf_metric_trasnf_dict, fair_metric_transf_dict (Dict): Dict with metrics as keys and list of metric value mitigation at different thresholds as values.\n",
        "        thresh_arr (Array): Array with different thresholds.\n",
        "        threshold (float, optional): Threshold to be indicated by a dotted line in the plot.\n",
        "        \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(14, 12))\n",
        "\n",
        "    ax1 = plt.subplot(2, 2, 1)\n",
        "    plot_metric_arr(ax1, perf_metric_dict, perf_metric_transf_dict, thresh=threshold, ylim=(0, 1))\n",
        "\n",
        "    fair_metric_values = list(fair_metric_dict.values()) + list(fair_metric_transf_dict.values())\n",
        "    min_val = min([v for val_list in fair_metric_values for v in val_list])\n",
        "    max_val = max([v for val_list in fair_metric_values for v in val_list])\n",
        "    custom_ylim = (round(min_val, 1)-0.1, round(max_val+0.1, 1))\n",
        "    ax2 = plt.subplot(2, 2, 2)\n",
        "    plot_metric_arr(ax2, fair_metric_dict, fair_metric_transf_dict, thresh=threshold, ylim=custom_ylim, hline_at=0)\n",
        "\n",
        "    fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jUDd9NVyEQ_"
      },
      "outputs": [],
      "source": [
        "def get_fairness_mitigation_over_thresholds(dataset_true, dataset_pred, dataset_transf_pred, thresh_arr, unprivileged_groups, privileged_groups, threshold=None):\n",
        "    \"\"\"Plots fairness metrics before and after mitigation over thresholds in the specified thresh_arr.\n",
        "\n",
        "    Args:\n",
        "        dataset_true (BinaryLabelDataset): Dataset with true labels.\n",
        "        dataset_pred (BinaryLabelDataset): Dataset with predicted scores.\n",
        "        dataset_transf_pred (BinaryLabelDataset): Dataset with predicted scores after mitigation.\n",
        "        thresh_arr (Array): Array with different thresholds.\n",
        "        unprivileged_groups (list(dict)): Unprivileged groups used as input for ClassificationMetric. For details, see AIF360 documentation.\n",
        "        privileged_groups (list(dict)): Privileged groups used as input for ClassificationMetric. For details, see AIF360 documentation.\n",
        "        threshold (float, optional): \n",
        "        \"\"\"\n",
        "    perf_metric_dict = defaultdict(list)\n",
        "    perf_metric_transf_dict = defaultdict(list)\n",
        "    fair_metric_dict = defaultdict(list)\n",
        "    fair_metric_transf_dict = defaultdict(list)\n",
        "\n",
        "    for i, thresh in enumerate(thresh_arr):\n",
        "        dataset_pred_thresh = get_label_at_threshold(dataset_pred, thresh)\n",
        "        dataset_transf_pred_thresh = get_label_at_threshold(dataset_transf_pred, thresh)\n",
        "\n",
        "        performance_metrics, fairness_metrics = get_classification_metrics(dataset_true, dataset_pred_thresh)\n",
        "\n",
        "        for (metric, value) in performance_metrics.items():\n",
        "            perf_metric_dict[metric].append(value)\n",
        "        \n",
        "        for (metric, value) in fairness_metrics.items():\n",
        "            fair_metric_dict[metric].append(value)\n",
        "\n",
        "        performance_metrics_transf, fairness_metrics_transf = get_classification_metrics(dataset_true, dataset_transf_pred_thresh)\n",
        "        \n",
        "        for (metric, value) in performance_metrics_transf.items():\n",
        "            perf_metric_transf_dict[metric].append(value)\n",
        "        \n",
        "        for (metric, value) in fairness_metrics_transf.items():\n",
        "            fair_metric_transf_dict[metric].append(value)\n",
        "    \n",
        "    print(\"The solid line indicates values before bias mitigation.\\nThe dashed line indicates values after mitigation.\")\n",
        "    plot_metrics_transf_over_thresholds(perf_metric_dict, perf_metric_transf_dict, fair_metric_dict, fair_metric_transf_dict, thresh_arr, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChrcUYd6CHEZ"
      },
      "outputs": [],
      "source": [
        "def show_variable_distr(df_var, x, hue=None, legend=None):\n",
        "    \"\"\"Plots distribution of variables in the given dataset.\n",
        "\n",
        "    Args:\n",
        "        df_var (DataFrame): Pandas DataFrame object.\n",
        "        x (str): Attribute to be plotted on the x-axis.\n",
        "        hue (str): Attribute to separate data on x-axis.\n",
        "        legend (list(str), optional): List of strings to use as legend values.\n",
        "        \"\"\"\n",
        "    df_distr = df_var[[x]] if not hue else df_orig.groupby(x)[hue]\n",
        "    df_distr = df_distr.value_counts(normalize=True)\n",
        "    df_distr = df_distr.mul(100)\n",
        "    df_distr = df_distr.rename('percent').reset_index()\n",
        "\n",
        "    g = sns.catplot(x=x,y='percent',hue=hue,kind='bar', data=df_distr, legend=False if legend else True)\n",
        "    g.ax.set_ylim(0,100)\n",
        "    if legend:\n",
        "        ax_legend = g.ax.legend(title=hue)\n",
        "        ax_legend.texts[0].set_text(legend[0])\n",
        "        ax_legend.texts[1].set_text(legend[1])\n",
        "\n",
        "    for p in g.ax.patches:\n",
        "        g.ax.annotate(f'\\n{round(p.get_height(),2)}%',\n",
        "                    (p.get_x() + (0.38 if not hue else 0.2),\n",
        "                    p.get_height()),\n",
        "                    ha='center',\n",
        "                    va='top', color='black',\n",
        "                    size=12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_dataset_distributions(dataset, dataset_pred=None):\n",
        "    \"\"\"Print distribution of target  and sens_attr in the given dataset and the dataset_pred if specified.\n",
        "\n",
        "    Args:\n",
        "        dataset (BinaryLabelDataset): Dataset to show distributions for. \n",
        "        dataset_pred (BinaryLabelDataset, optional): If not None, show distribution for this dataset.\n",
        "        \"\"\"\n",
        "    df = dataset.convert_to_dataframe()[0]\n",
        "    display(Markdown(f\"#### Dataset {target} distribution\"))\n",
        "    print(df.value_counts(target, normalize=True).round(4))\n",
        "    display(Markdown(f\"#### Dataset {sens_attr} distribution\"))\n",
        "    print(df.value_counts(sens_attr, normalize=True).round(4))\n",
        "    display(Markdown(f\"#### Dataset {target} x {sens_attr} distribution\"))\n",
        "    print(pd.crosstab(df[target],df[sens_attr], normalize=True).round(4))\n",
        "\n",
        "    if dataset_pred:\n",
        "        df_pred = dataset_pred.convert_to_dataframe()[0]\n",
        "        target_pred = f'{target}_pred'\n",
        "        df_pred = df_pred.rename(columns={target: target_pred})\n",
        "        display(Markdown(f\"#### Dataset {target} predicted distribution\"))\n",
        "        print(df_pred.value_counts(target_pred, normalize=True).round(4))\n",
        "        display(Markdown(f\"#### Dataset {target} x {sens_attr} predicted distribution\"))\n",
        "        print(pd.crosstab(df_pred[target_pred],df_pred[sens_attr], normalize=True).round(4))"
      ],
      "metadata": {
        "id": "yz0WROynTQFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_conf_matrix(dataset_true, dataset_pred, per_group=True):\n",
        "    \"\"\"Print confusion matrix for given datasets as markdown table. If per_group is True, confusion matrix will be shown for each subgroup of the sensitive attribute.\n",
        "\n",
        "    Args:\n",
        "        dataset, dataset_pred (BinaryLabelDataset): Datasets to show confusion matrix for. \n",
        "        per_group (bool, optional): If yes, show confusion matrix for each subgroup of the sensitive attribute.\n",
        "        \"\"\"\n",
        "\n",
        "    conf_matrix_dict = OrderedDict()\n",
        "\n",
        "    if per_group:\n",
        "        df_true = dataset_true.convert_to_dataframe()[0]\n",
        "        df_pred = dataset_pred.convert_to_dataframe()[0]\n",
        "        for value in df_true[sens_attr].unique():\n",
        "            df_true_group = df_true[df_true[sens_attr] == value]\n",
        "            df_pred_group = df_pred[df_pred[sens_attr] == value]\n",
        "            conf_matrix = confusion_matrix(df_true_group[target], df_pred_group[target], normalize='all')\n",
        "            conf_matrix_dict[value] = conf_matrix\n",
        "    else:\n",
        "        conf_matrix_dict[0] = confusion_matrix(dataset_true.labels, dataset_pred.labels, normalize='all')\n",
        "    \n",
        "    for (k, conf_matrix) in conf_matrix_dict.items():\n",
        "        if per_group:\n",
        "            display(Markdown(f\"#### Confusion matrix for {sens_attr} = {k}\"))\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        df_metrics = pd.DataFrame({'':['LTU', 'Non LTU'],\n",
        "                    'Pred LTU':[tp, fp],\n",
        "                    'Pred non LTU':[fn, tn]})\n",
        "        \n",
        "        print(df_metrics.round(4).to_markdown(index=False))"
      ],
      "metadata": {
        "id": "yu_WKwIz9WxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_dataset_metrics(dataset):\n",
        "    \"\"\"Show dataset metrics for given dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (BinaryLabelDataset): Dataset to get dataset metrics from. \n",
        "        \"\"\"\n",
        "    show_dataset_distributions(dataset)\n",
        "\n",
        "    dataset_metric = BinaryLabelDatasetMetric(\n",
        "        dataset,\n",
        "        unprivileged_groups=unprivileged_groups,\n",
        "        privileged_groups=privileged_groups)\n",
        "    \n",
        "    metric_dict = OrderedDict()\n",
        "    metric_dict[\"Statistical parity difference\"] = dataset_metric.statistical_parity_difference()\n",
        "\n",
        "    display(Markdown(f\"#### Dataset metrics\"))\n",
        "    for k in metric_dict:\n",
        "            print(f\"{k} = {metric_dict[k]:.4f}\")"
      ],
      "metadata": {
        "id": "OdoGCPpOJuMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_classification_metrics(dataset_true, dataset_pred, f1=True):\n",
        "    \"\"\"Show classification metrics for given datasets.\n",
        "\n",
        "    Args:\n",
        "        dataset, dataset_pred (BinaryLabelDataset): Dataset to get classification metrics from. \n",
        "        f1 (bool, optional): If True, get F1 score.\n",
        "        \"\"\"\n",
        "    show_dataset_distributions(dataset_true, dataset_pred)\n",
        "    show_conf_matrix(dataset_true, dataset_pred) \n",
        "   \n",
        "    performance_metrics_dict, fairness_metrics_dict = get_classification_metrics(dataset_true, dataset_pred, f1)\n",
        "    classification_metrics_dict = dict(performance_metrics_dict, **fairness_metrics_dict)\n",
        "\n",
        "    display(Markdown(f\"#### Classification metrics\"))\n",
        "    for k in classification_metrics_dict:\n",
        "        print(f\"{k} = {classification_metrics_dict[k]:.4f}\")"
      ],
      "metadata": {
        "id": "eHc1izTiKv0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvJjaZfF7li5"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the data generation phase, the data used for the prediction task is gathered, selected and, if necessary, labeled. The labels are later used to assess the \"true outcome\" of the variable to be predicted. In our case, this is the probability of long term unemployment (LTU)."
      ],
      "metadata": {
        "id": "XFhJXrPcpsn2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZ4PndAAg5R"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, a dataset from [GitHub](https://github.com/), a code sharing platform and version control tool, is loaded into our notebook. Then, we output the head (first five rows) of the dataframe (like a data table) to get a first impression of the data set at hand."
      ],
      "metadata": {
        "id": "Ms7RJ_JfrVeh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXq0mOdzK6wb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_orig = pd.read_csv('https://raw.githubusercontent.com/achterhe/fair-ml-goethe/main/daten.csv', index_col=0)\n",
        "df_orig.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xDthqoo7rRH"
      },
      "source": [
        "# Data preparation and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, the data is first prepared to be later used as input for the classification model. The sensitive attribute is postulated, variable distributions are explored and model decisions, like the classification threshold, are specified."
      ],
      "metadata": {
        "id": "NFHOnfiqsK-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HikZl4pCAaHs"
      },
      "outputs": [],
      "source": [
        "# Set relevant variables\n",
        "\n",
        "sens_attr = 'gender'\n",
        "\n",
        "privileged_groups = [{sens_attr: 0}] # Male\n",
        "unprivileged_groups = [{sens_attr: 1}] # Female\n",
        "\n",
        "target = 'ltue'\n",
        "\n",
        "threshold = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_McJrJ5M8Et"
      },
      "source": [
        "## Analyze the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain insight into the available data, we plot the distribution of the target variable and the sensitive attribute."
      ],
      "metadata": {
        "id": "qQvB8fU5DA_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2gr-0kplJIW"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"#### Distribution of target variable\"))\n",
        "show_variable_distr(df_orig, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8BPNR87Nhjh"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"#### Distribution of sensitive attribute\"))\n",
        "show_variable_distr(df_orig, sens_attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QetmpOigj3BQ"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"#### Distribution of target per sensitive attribute\"))\n",
        "show_variable_distr(df_orig, target, hue=sens_attr, legend=['M', 'F'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXjREPhBR9oh"
      },
      "source": [
        "## Train-Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we convert our data into a BinaryLabelDataset, a dataset type that is required for further processing of the data with the AIF360 toolkit. At this point, you may wonder about the \"favorable_label\" parameter, which we have set to 1 for this use case. Remember that ltue=1 means that the person is actually LTU. So is this really the favorable outcome? A look at the [documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset) of the BinaryLabelDataset class shows that \"favorable\" is used here in the same way as \"positive outcome\", which is more common in ML and denotes predictions that are classified as 1. However, this does not explicitly indicate whether this prediction is the desired one, but depends on interpretation. Therefore, we criticize the use of \"favorable\" and \"unfavorable\" at this point."
      ],
      "metadata": {
        "id": "0nDpyoziDmoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOHM6xDRSN_0"
      },
      "outputs": [],
      "source": [
        "# Create a BinaryLabelDataset\n",
        "\n",
        "dataset_orig = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=df_orig,\n",
        "    label_names=[target],\n",
        "    protected_attribute_names=[sens_attr]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we split the data into a training set and a test set. A validation set is not considered for this use case. \n",
        "With a split parameter of 0.7, 70% of the instances will belong to the training data set, while 30% will be used for the test data set. By splitting the data, we are able to evaluate the performance of our model. The model learns relationships between the features in the training dataset and the true label. Next, the trained model is used to predict the results of the test data set. To evaluate the model, we then compare the predictions to the expected label we have in the test data."
      ],
      "metadata": {
        "id": "1KWhukmrHim-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BThOZbl2SvTN"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, validation and test set\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the split, we can explore the training data set again"
      ],
      "metadata": {
        "id": "ojZlQfUZJofR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOpoW-rFTEQS"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"#### Training Dataset shape\"))\n",
        "print(dataset_orig_train.features.shape)\n",
        "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
        "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
        "display(Markdown(\"#### Protected attribute names\"))\n",
        "print(dataset_orig_train.protected_attribute_names)\n",
        "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
        "print(dataset_orig_train.privileged_protected_attributes,\n",
        "      dataset_orig_train.unprivileged_protected_attributes)\n",
        "display(Markdown(\"#### Dataset feature names\"))\n",
        "print(dataset_orig_train.feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning a classifier"
      ],
      "metadata": {
        "id": "E-FiKnDsJ3YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train a classifier with logistic regression on the test data and then make predictions on the test data. Performance evaluation can be done using the confusion matrix and the classification report."
      ],
      "metadata": {
        "id": "1ykPtlT1J5xd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwduBXDyTO9B"
      },
      "outputs": [],
      "source": [
        "lmod = LogisticRegression(random_state=seed, solver=\"liblinear\")\n",
        "lmod.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_2jVb5zT_K6"
      },
      "outputs": [],
      "source": [
        "dataset_orig_test_pred = get_dataset_with_predictions(dataset_orig_test, lmod, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRB8sgsIUF_5"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Confustion matrix\"))\n",
        "show_conf_matrix(dataset_orig_test, dataset_orig_test_pred, per_group=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLYGudSqUdED"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Classification report\"))\n",
        "print(classification_report(dataset_orig_test.labels, dataset_orig_test_pred.labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gncYtsnXRTd"
      },
      "source": [
        "## Calculate fairness metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, the notion of fairness comes into play. Here we can distinguish between [dataset metrics](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html#aif360.metrics.BinaryLabelDatasetMetric), which can be derived from a single dataset, and [classification metrics](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html), which take into account the differences between true outcomes and predictions.\n",
        "\n",
        "First, let's look at the metrics of our original test data set including the true outcome."
      ],
      "metadata": {
        "id": "YooAOdj3KzeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yHvH3Nlwk_y"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"## Original test dataset\"))\n",
        "show_dataset_metrics(dataset_orig_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Try to calculate the Statistical Parity Difference on your own by using the values \n",
        "given in the distribution tables above. Remember, gender=1 indicate female instances.\n",
        "\n",
        "SPD = actually LTU women / all women - actually LTU men / all men\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "act_ltu_women = 0\n",
        "all_women = 1\n",
        "act_ltu_men = 0\n",
        "all_men = 1\n",
        "\n",
        "print(act_ltu_women/all_women-act_ltu_men/all_men)"
      ],
      "metadata": {
        "id": "ODiSlUxWVyVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now look at the classification metrics after predicting the outcome with the trained model. "
      ],
      "metadata": {
        "id": "exygP3T8Sy_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpLH3UIzXoBV"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"## Predicted test dataset\"))\n",
        "show_classification_metrics(dataset_orig_test, dataset_orig_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Try to calculate the Equal Opportunity Difference on your own by using the values \n",
        "given in the confusion matrices above. Remember, gender=1 indicate female instances.\n",
        "\n",
        "EOD = True Positive women / all actual positive women - True Positive men / all actual positive men\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "tp_women = 0\n",
        "all_pos_women = 1\n",
        "tp_men = 0\n",
        "all_pos_men = 1\n",
        "\n",
        "print(tp_women/all_pos_women-tp_men/all_pos_men)"
      ],
      "metadata": {
        "id": "_aw2408MXxaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the beginning, we set the classification threshold to 0.5, which means that every instance where the model predicts ltue=1 with a probability greater than 0.5 is assigned ltue_pred=1. In the following cell, we plot the F1 score and the fairness metrics across different thresholds to gain insight into how much the threshold affects fairness."
      ],
      "metadata": {
        "id": "BuEZ8sCeQ5JA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqEXwLJIkt1n"
      },
      "outputs": [],
      "source": [
        "thresh_arr = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "get_fairness_over_thresholds(dataset_orig_test, lmod, thresh_arr, unprivileged_groups, privileged_groups, threshold=threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3HhEOHAH_5c"
      },
      "source": [
        "## Removing the sensitive attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the algorithm is found to make its decision depending on a protected attribute, it appears obvious to simply remove this feature from the dataset to correct for the bias. So, let's omit the variable gender during training and look at the fairness metrics."
      ],
      "metadata": {
        "id": "gZGhAJdzTO6c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRUvUb-lICeV"
      },
      "outputs": [],
      "source": [
        "# Remove the variable gender from the datasets and convert them to BinaryLabelDatasets again\n",
        "\n",
        "df_train = dataset_orig_train.convert_to_dataframe()[0]\n",
        "df_train_ng = df_train.drop([sens_attr], axis=1)\n",
        "\n",
        "df_test = dataset_orig_test.convert_to_dataframe()[0]\n",
        "df_test_ng = df_test.drop([sens_attr], axis=1)\n",
        "\n",
        "dataset_train_ng = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=df_train_ng,\n",
        "    label_names=[target],\n",
        "    protected_attribute_names=[]\n",
        ")\n",
        "\n",
        "dataset_test_ng = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=df_test_ng,\n",
        "    label_names=[target],\n",
        "    protected_attribute_names=[]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC0f4f8dNVfj"
      },
      "outputs": [],
      "source": [
        "# Train a Logistic Regression on the data without gender\n",
        "\n",
        "lmod_ng = LogisticRegression(random_state=seed, solver=\"liblinear\")\n",
        "lmod_ng.fit(dataset_train_ng.features, dataset_train_ng.labels.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pesg3WzXN68A"
      },
      "outputs": [],
      "source": [
        "# Predict the test data set with the new model\n",
        "\n",
        "dataset_test_ng_pred = get_dataset_with_predictions(dataset_test_ng, lmod_ng, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQJTJYpNN68B"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Confustion matrix\"))\n",
        "show_conf_matrix(dataset_orig_test, dataset_test_ng_pred, per_group=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MdF46oCN68D"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Classification report\"))\n",
        "print(classification_report(dataset_test_ng.labels, dataset_test_ng_pred.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR5M4GnYPoaC"
      },
      "outputs": [],
      "source": [
        "# To examine the differences between genders, we create a dataset with gender assigned the scores and labels we predicted previously\n",
        "\n",
        "dataset_test_ng_pred_wg = dataset_orig_test.copy(deepcopy=True)\n",
        "dataset_test_ng_pred_wg.scores = dataset_test_ng_pred.scores\n",
        "dataset_test_ng_pred_wg.labels = dataset_test_ng_pred.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqkLVKP1PCCK"
      },
      "outputs": [],
      "source": [
        "dataset_test_ng_metric = ClassificationMetric(dataset_orig_test,\n",
        "        dataset_test_ng_pred_wg,\n",
        "        unprivileged_groups=unprivileged_groups,\n",
        "        privileged_groups=privileged_groups)\n",
        "\n",
        "display(Markdown(\"## Predicted test dataset without sensitive attribute\"))\n",
        "show_classification_metrics(dataset_orig_test, dataset_test_ng_pred_wg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_fairness_over_thresholds(dataset_orig_test, lmod, thresh_arr, unprivileged_groups, privileged_groups, threshold=threshold, dataset_with_scores=dataset_test_ng_pred_wg)"
      ],
      "metadata": {
        "id": "HprRHKOH9LKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add-on"
      ],
      "metadata": {
        "id": "TTYNPFeGbKI1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjeaRqu7v9Nw"
      },
      "source": [
        "## Bias mitigation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the use of bias mitigation algorithms, we apply reweighing to the data at hand. This algorithm assigns a weight to each instance with respect to its sens_attr value and its true outcome. A classifier is then trained with the new instance weights."
      ],
      "metadata": {
        "id": "AXgnn2JkbPgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cottpHn9lf76"
      },
      "outputs": [],
      "source": [
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "               privileged_groups=privileged_groups)\n",
        "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv2SSSiaxIXu"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"## Transformed training dataset\"))\n",
        "show_dataset_metrics(dataset_transf_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleLjXTvxhh-"
      },
      "source": [
        "### Train classifier on transformed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9kp8tjsxUzw"
      },
      "outputs": [],
      "source": [
        "lmod_transf = LogisticRegression(random_state=seed, solver=\"liblinear\")\n",
        "lmod_transf = lmod_transf.fit(dataset_transf_train.features, dataset_transf_train.labels.ravel(), sample_weight=dataset_transf_train.instance_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUFq_oI2xrB4"
      },
      "outputs": [],
      "source": [
        "dataset_transf_test_pred = get_dataset_with_predictions(dataset_orig_test, lmod_transf, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(f\"## Confustion matrix\"))\n",
        "show_conf_matrix(dataset_orig_test, dataset_transf_test_pred, per_group=False)"
      ],
      "metadata": {
        "id": "ymp4L1HZCFjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(f\"## Classification report\"))\n",
        "print(classification_report(dataset_orig_test.labels, dataset_transf_test_pred.labels))"
      ],
      "metadata": {
        "id": "8-GqVh_wCNVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6vlCCaqFdBz"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"## Predicted transformed test dataset\"))\n",
        "show_classification_metrics(dataset_orig_test, dataset_transf_test_pred, f1=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiiUPrYvx31E"
      },
      "outputs": [],
      "source": [
        "get_fairness_mitigation_over_thresholds(dataset_orig_test, dataset_orig_test_pred, dataset_transf_test_pred, thresh_arr, unprivileged_groups, privileged_groups, threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying a Random Forest model"
      ],
      "metadata": {
        "id": "ghg5k4i6Q-ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, it is possible to use a model other than logistic regression for this classification problem. The following cells show the use of a Random Forest classifier and the resulting fairness evaluation."
      ],
      "metadata": {
        "id": "qhVExGEEcKdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Random Forest Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "rf=RandomForestClassifier(n_estimators=100, random_state=seed)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "rf.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel())"
      ],
      "metadata": {
        "id": "voOiBHPeRBS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey66H7XEdUjO"
      },
      "outputs": [],
      "source": [
        "dataset_orig_test_pred_rf = get_dataset_with_predictions(dataset_orig_test, rf, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJBYiCbCdUjP"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Confustion matrix\"))\n",
        "show_conf_matrix(dataset_orig_test, dataset_orig_test_pred_rf, per_group=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxFFeK9NdUjQ"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"## Classification report\"))\n",
        "print(classification_report(dataset_orig_test.labels, dataset_orig_test_pred_rf.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fOyS5k2dUjV"
      },
      "outputs": [],
      "source": [
        "display(Markdown(\"## Predicted test dataset\"))\n",
        "show_classification_metrics(dataset_orig_test, dataset_orig_test_pred_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDy-yzOZdUjX"
      },
      "outputs": [],
      "source": [
        "thresh_arr = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "get_fairness_over_thresholds(dataset_orig_test, rf, thresh_arr, unprivileged_groups, privileged_groups, threshold=threshold)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9S17h70QdlPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}